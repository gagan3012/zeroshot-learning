{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Zero Shot learning experiment.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPtgWslzbshuGIZldABw57I",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gagan3012/zeroshot-learning/blob/master/Zero_Shot_learning_experiment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m9Xl0NnDx0NI"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"/content/bquxjob_3bc46ddf_17ce26263bb.csv\")\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C35EOzq63UoA"
      },
      "source": [
        "!pip install newspaper3k==0.2.8"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tYTt3_Fm03u3"
      },
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import newspaper\n",
        "\n",
        "\n",
        "def get_article_metadata_newspaper(df):\n",
        "    \"\"\"\n",
        "    Get/extract the metadata of news article, including, article heading and main content, from it's URL.\n",
        "    Arguments:\n",
        "        url: (str) link of news article (starting with http:// or https://) \n",
        "    Returns:\n",
        "        dict:\n",
        "            title: (str) extracted heading of news article\n",
        "            text: (str) extracted main content of news article \n",
        "    \"\"\"\n",
        "\n",
        "    df = df.drop_duplicates(subset=['SOURCEURL'],ignore_index = True)\n",
        "    df[\"text\"] = 0\n",
        "    df[\"title\"]= 0\n",
        "\n",
        "    for i in range(len(df)):\n",
        "      url = df['SOURCEURL'][i]\n",
        "      article = newspaper.Article(url)\n",
        "\n",
        "      try:\n",
        "        article.download()\n",
        "        article.parse()\n",
        "      except:\n",
        "        continue\n",
        "\n",
        "      df[\"text\"][i] = article.text\n",
        "      df[\"title\"][i] = article.title\n",
        " \n",
        "    return df\n",
        "    "
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ek79OLEw3Lbq"
      },
      "source": [
        "df_new = get_article_metadata_newspaper(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7kUvF0d06XmV"
      },
      "source": [
        "df_new.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ESG4tOpDzjg",
        "outputId": "98f43163-3b65-4792-e229-be5078af0215"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.12.2)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.0.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.46)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.17->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4U_x523DIb8n",
        "outputId": "3e56fe5b-c384-4731-e535-1eb3be858537"
      },
      "source": [
        "len(df_new[\"text\"][0])"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8742"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-2PLWQqQkNVA"
      },
      "source": [
        "import re\n",
        "def stringupdate(i):\n",
        "  strList = df_new[\"Themes\"][i].split(\";\")\n",
        "\n",
        "  strList = list(map(lambda x: str(x).replace('WB_', ''), strList))\n",
        "\n",
        "  strList = list(map(lambda x: re.sub('^\\d+', '', x), strList))\n",
        "\n",
        "  strList = list(map(lambda x: str(x).replace('_', '', 1), strList))\n",
        "\n",
        "  strList = list(filter(None, strList))\n",
        "\n",
        "  return strList"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tai__nZ0tHlN"
      },
      "source": [
        "##### ZSL\n",
        "This is the checkpoint for bart-large after being trained on the MultiNLI (MNLI) dataset.\n",
        "\n",
        "Additional information about this model:\n",
        "\n",
        "The bart-large model page\n",
        "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension\n",
        "BART fairseq implementation\n",
        "NLI-based Zero Shot Text Classification\n",
        "Yin et al. proposed a method for using pre-trained NLI models as a ready-made zero-shot sequence classifiers. The method works by posing the sequence to be classified as the NLI premise and to construct a hypothesis from each candidate label. For example, if we want to evaluate whether a sequence belongs to the class \"politics\", we could construct a hypothesis of This text is about politics.. The probabilities for entailment and contradiction are then converted to label probabilities.\n",
        "\n",
        "This method is surprisingly effective in many cases, particularly when used with larger pre-trained models like BART and Roberta. See this blog post for a more expansive introduction to this and other zero shot methods, and see the code snippets below for examples of using this model for zero-shot classification both with Hugging Face's built-in pipeline and with native Transformers/PyTorch code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sZJosrMtB88E"
      },
      "source": [
        "from transformers import pipeline\n",
        "classifier = pipeline(\"zero-shot-classification\",\n",
        "                      model=\"facebook/bart-large-mnli\")"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xg2w8OnEDy0m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "803e3cdf-f50a-4db6-98ca-dc66cbb8a6ea"
      },
      "source": [
        "sequence_to_classify =df_new[\"title\"][3]\n",
        "candidate_labels = stringupdate(3)\n",
        "classifier(sequence_to_classify, candidate_labels, multi_label=True)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'labels': ['TAXWORLDMAMMALS_DOG',\n",
              "  'TAXETHNICITY_CHINESE',\n",
              "  'CRISISLEXC03_WELLBEING_HEALTH',\n",
              "  'TAXWORLDLANGUAGES_CHINESE',\n",
              "  'TRANSPORT',\n",
              "  'PUBLIC_SECTOR_MANAGEMENT',\n",
              "  'MOVEMENTGENERAL',\n",
              "  'UNGPFORESTS_RIVERS_OCEANS',\n",
              "  'TAXFNCACT_COMPANION',\n",
              "  'TRANSPORT_INFRASTRUCTURE',\n",
              "  'JUSTICE',\n",
              "  'CRIMINAL_JUSTICE',\n",
              "  'TAXWORLDMAMMALS_HUMANS',\n",
              "  'GENERALHEALTH',\n",
              "  'TAXWORLDLANGUAGES',\n",
              "  'TAXETHNICITY',\n",
              "  'TAXWORLDMAMMALS',\n",
              "  'PORTS',\n",
              "  'TAXFNCACT',\n",
              "  'GENERALGOVERNMENT',\n",
              "  'MEDICAL'],\n",
              " 'scores': [0.41150563955307007,\n",
              "  0.17527413368225098,\n",
              "  0.17282062768936157,\n",
              "  0.1562877595424652,\n",
              "  0.07622707635164261,\n",
              "  0.07147242873907089,\n",
              "  0.06926321238279343,\n",
              "  0.05468777194619179,\n",
              "  0.04741927981376648,\n",
              "  0.04313568398356438,\n",
              "  0.026548730209469795,\n",
              "  0.020269725471735,\n",
              "  0.01475998479872942,\n",
              "  0.011680550873279572,\n",
              "  0.011095426045358181,\n",
              "  0.008031290955841541,\n",
              "  0.0049979970790445805,\n",
              "  0.004462936893105507,\n",
              "  0.0044412254355847836,\n",
              "  0.003389760386198759,\n",
              "  0.0009070485248230398],\n",
              " 'sequence': 'The viral Xiaomi robotic dog posed to be an affordable challenge to Boston Dynamic’s Spot just released new images + sketches!'}"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mCGijDkYSoWO"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}